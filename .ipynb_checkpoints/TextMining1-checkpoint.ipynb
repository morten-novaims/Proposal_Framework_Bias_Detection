{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/morten-novaims/Text_Mining_HW/blob/master/TextMining1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Vk9JZzfhVMi"
   },
   "source": [
    "# Text Mining - Class 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Yk_wftShsjn"
   },
   "source": [
    "### Basic text processing (Grep exercise)\n",
    "\n",
    "Download the document https://drive.google.com/file/d/1ESCJYl2qa-xF-\n",
    "rCWEaL-eQ2Spp_JAEh-/view?usp=sharing and explore the grep/awk command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eApWgsdRGx_Q"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "24LgANY_K5z-",
    "outputId": "6b8f4838-bdd7-4b00-ec67-04df356ee2ac"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e2a8a0eb45d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1ESCJYl2qa-xF-rCWEaL-eQ2Spp_JAEh-'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownloaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetContentString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "# Download a file based on its file ID.\n",
    "#\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
    "file_id = '1ESCJYl2qa-xF-rCWEaL-eQ2Spp_JAEh-'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "wiki = downloaded.GetContentString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "SUQa3rYoLM_n",
    "outputId": "3c747a37-ee02-4ece-f7f9-01ed68f915ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!\n"
     ]
    }
   ],
   "source": [
    "# Create a local file to perfr.\n",
    "with open('wiki.txt', 'w') as f:\n",
    "  f.write(wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfXnhcRAhzha"
   },
   "source": [
    "1. Find how many lines exist in the document.  \n",
    "1. Count how many time each word occurs:  \n",
    "  1. mining  \n",
    "  1. textual  \n",
    "  1. term  \n",
    "1. (Optional extra 10 points) Extract only the last word of each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjZsu3rKuEQe"
   },
   "source": [
    "__*grep and awk not working in colab*__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "u045pDGlMCuT",
    "outputId": "48471e7b-a2b1-413e-874d-c19dadd5c511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines\n",
      "128 wiki.txt\n",
      "How many times mining appears\n",
      "How many times textual appears\n",
      "How many times term appers\n",
      "The last word of each line:\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!echo \"Total lines\"\n",
    "!wc -l wiki.txt\n",
    "!echo \"How many times mining appears\"\n",
    "!grep -o mining | wc -l\n",
    "!echo \"How many times textual appears\"\n",
    "!grep -o textual | wc -l\n",
    "!echo \"How many times term appers\"\n",
    "!grep -o term | wc -l\n",
    "!echo \"The last word of each line:\"\n",
    "!awk 'NF>1{print $NF}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5pWhTNTik39"
   },
   "source": [
    "### Basic Text Processing (re module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JhxVNKQPDvfy"
   },
   "source": [
    "Get to know your regex:\n",
    "\n",
    "For the sentence:\n",
    "“ :) A smile to end up your day with. =) What is making you happy :))))))\n",
    "these days? <3 :DDDD ”\n",
    "\n",
    "1.1 Find every occurrence of a smiley :), ;), =), :)))))), :D, :DDDD and\n",
    "replace it with the word happy\n",
    "\n",
    "1.2 Find every occurrence of a number and replace it with the word digit\n",
    "in this document: https://drive.google.com/file/d/\n",
    "1hTJGWBXgVSBZlZ8klcVz7T59LeH7ncIq/view?usp=sharing\n",
    "\n",
    "1.3 Create a regex for identifying punctuation marks (all kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Q7sHY-LE5US"
   },
   "outputs": [],
   "source": [
    "## import the regex module first to use regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "eKlWP5J7ikTU",
    "outputId": "11e530a1-fd29-485d-e6e6-7c9176fc6fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " happy A smile to end up your day with. happy What is making you happy happy these days? <3 happy \n"
     ]
    }
   ],
   "source": [
    "## Question 1.1: Find every occurrence of a smiley :), ;), =), :)))))), :D, :DDDD and replace it with the word happy\n",
    "\n",
    "# add the sentence and the characters into a variable\n",
    "sentence = \" :) A smile to end up your day with. =) What is making you happy :)))))) these days? <3 :DDDD \"\n",
    "\n",
    "# setting up the regex\n",
    "characters =  r\"[:;=][\\)D]+\"\n",
    "\n",
    "# substitute the characters with happy using a join operator -> | represents a logical OR\n",
    "sub_sentence = re.sub(characters, \"happy\", sentence)\n",
    "print(sub_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694
    },
    "colab_type": "code",
    "id": "d2ClT9gnnVtX",
    "outputId": "dbca5430-d7a2-4f04-9b44-acdba0df8a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India (IAST: Bhārat), also known as the Republic of India (IAST: BhāratGaṇarājya),\n",
      "[digit[e]\n",
      "is a country in South Asia. It is the seventh largest country by area\n",
      "and with more than digitdigitbillion people, it is the second most populouscountry and\n",
      "the most populous democracy in the world. Bounded by the Indian Ocean on the\n",
      "south, the Arabian Seaon the southwest, and the Bay of Bengal on the southeast,\n",
      "it shares land borders with Pakistan to the west;\n",
      "\n",
      "[f] China, Nepal, and Bhutan to the\n",
      "northeast; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is\n",
      "in the vicinity of Sri Lanka and the Maldives, while its Andaman and Nicobar\n",
      "Islands share a maritime border with Thailandand Indonesia.\n",
      "The Indian subcontinent was home to the urban Indus Valley Civilisation of the digitd\n",
      "millennium BCE. In the following millennium, the oldest scriptures associated\n",
      "with Hinduism began to be composed. Social stratification, based on caste,\n",
      "emerged in the first millennium BCE, and Buddhism and Jainism arose. Early\n",
      "political consolidations took place under the Maurya and Gupta empires; later\n",
      "peninsular Middle Kingdoms influenced cultures as far as Southeast Asia. In the\n",
      "medieval era, Judaism, Zoroastrianism, Christianity, and Islam arrived,\n",
      "and Sikhism emerged, all adding to the region's diverse culture. Much of the north\n",
      "fell to the Delhi Sultanate; the south was united under the Vijayanagara Empire.\n",
      "The economy expanded in the digith century in the Mughal Empire. In the mid-digith\n",
      "century, the subcontinent came under British East India Company rule, and in the\n",
      "mid-digith under British crown rule. A nationalist movement emerged in the late\n",
      "digith century, which later, under Mahatma Gandhi, was noted for nonviolent\n",
      "resistance and led to India's independence in digit\n",
      "In digit the Indian economy was the world's sixth largest by\n",
      "nominal GDP[digit and third largest by purchasing power parity.\n",
      "\n",
      "[digit Following market-\n",
      "based economic reforms in digit India became one of the fastest-growing major\n",
      "\n",
      "economies and is considered a newly industrialised country. However, it continues\n",
      "to face the challenges of poverty, corruption, malnutrition, and inadequate public\n",
      "healthcare. A nuclear weapons state and regional power, it has the second largest\n",
      "standing army in the world and ranks fifth in military expenditure among nations.\n",
      "India is a federal republic governed under a parliamentary system and consists\n",
      "of digitstates and digitunion territories. A pluralistic, multilingual and multi-ethnic\n",
      "society, it is also home to a diversity of wildlife in a variety of protected habitats.\n"
     ]
    }
   ],
   "source": [
    "## Question 1.2: Find every occurrence of a number and replace it with the word digit in this document\n",
    "\n",
    "# As I'm currently not able to get this stupid rtf converter tool up and running, this exercise is done manually\n",
    "# Question: instances such as \"sixth\" or \"millenium\" are they numbers? Answer: No, they are not!\n",
    "# Question: 18 should be replaced by digit or digitdigit? Answer: By digit of course.\n",
    "\n",
    "text = \"\"\"India (IAST: Bhārat), also known as the Republic of India (IAST: BhāratGaṇarājya),\n",
    "[18][e]\n",
    "is a country in South Asia. It is the seventh largest country by area\n",
    "and with more than 1.3 billion people, it is the second most populouscountry and\n",
    "the most populous democracy in the world. Bounded by the Indian Ocean on the\n",
    "south, the Arabian Seaon the southwest, and the Bay of Bengal on the southeast,\n",
    "it shares land borders with Pakistan to the west;\n",
    "\n",
    "[f] China, Nepal, and Bhutan to the\n",
    "northeast; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is\n",
    "in the vicinity of Sri Lanka and the Maldives, while its Andaman and Nicobar\n",
    "Islands share a maritime border with Thailandand Indonesia.\n",
    "The Indian subcontinent was home to the urban Indus Valley Civilisation of the 3rd\n",
    "millennium BCE. In the following millennium, the oldest scriptures associated\n",
    "with Hinduism began to be composed. Social stratification, based on caste,\n",
    "emerged in the first millennium BCE, and Buddhism and Jainism arose. Early\n",
    "political consolidations took place under the Maurya and Gupta empires; later\n",
    "peninsular Middle Kingdoms influenced cultures as far as Southeast Asia. In the\n",
    "medieval era, Judaism, Zoroastrianism, Christianity, and Islam arrived,\n",
    "and Sikhism emerged, all adding to the region's diverse culture. Much of the north\n",
    "fell to the Delhi Sultanate; the south was united under the Vijayanagara Empire.\n",
    "The economy expanded in the 17th century in the Mughal Empire. In the mid-18th\n",
    "century, the subcontinent came under British East India Company rule, and in the\n",
    "mid-19th under British crown rule. A nationalist movement emerged in the late\n",
    "19th century, which later, under Mahatma Gandhi, was noted for nonviolent\n",
    "resistance and led to India's independence in 1947.\n",
    "In 2017, the Indian economy was the world's sixth largest by\n",
    "nominal GDP[19] and third largest by purchasing power parity.\n",
    "\n",
    "[15] Following market-\n",
    "based economic reforms in 1991, India became one of the fastest-growing major\n",
    "\n",
    "economies and is considered a newly industrialised country. However, it continues\n",
    "to face the challenges of poverty, corruption, malnutrition, and inadequate public\n",
    "healthcare. A nuclear weapons state and regional power, it has the second largest\n",
    "standing army in the world and ranks fifth in military expenditure among nations.\n",
    "India is a federal republic governed under a parliamentary system and consists\n",
    "of 29 states and 7 union territories. A pluralistic, multilingual and multi-ethnic\n",
    "society, it is also home to a diversity of wildlife in a variety of protected habitats.\"\"\"\n",
    "\n",
    "sub_text = re.sub(r'\\d+.', \"digit\", text)\n",
    "print(sub_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LUccYdgpf7lV",
    "outputId": "29f953cc-c922-4ba6-85cc-15ac8c68c93e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[',', \"'\", '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Question 1.3: Create a regex for identifying punctuation marks (all kinds)\n",
    "\n",
    "# Idea: create a regex, that returns all possible punctuation marks, with various exceptions:\n",
    "# E.g. a numeric delimiter (0.1 is not really punctuation)\n",
    "\n",
    "sentence = 'Hi there, i\\'m looking for punctuation with 0.0 accuracy.'\n",
    "regex_punct = r'\\.(?!\\d)|\\,|\\!|\\?|\\|\\:|\\;|\\''\n",
    "re.findall(regex_punct, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qchfOW8wiqfQ"
   },
   "source": [
    "#### Basic Text Processing (pre-processing pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "xR1PUahqi7kh",
    "outputId": "7f4d1a2d-02f8-47ce-aa88-5979c23cac2a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "categories = ['sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "sentence = twenty_train['data'][15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "colab_type": "code",
    "id": "CGzq9Av8mggy",
    "outputId": "fd2d7013-a6db-4a6d-aa8b-32570c33488d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> punkt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package punkt to /home/jovyan/nltk_data...\n",
      "      Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "      Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n",
      "Word net Lemmatizer\n",
      "Actual: was  Lemma: wa\n",
      "Actual: as  Lemma: a\n",
      "Actual: intents  Lemma: intent\n",
      "Actual: purposes  Lemma: purpose\n",
      "Actual: gets  Lemma: get\n",
      "Actual: gets  Lemma: get\n",
      "Actual: physicians  Lemma: physician\n",
      "Actual: was  Lemma: wa\n",
      "Actual: years  Lemma: year\n",
      "Actual: as  Lemma: a\n",
      "Actual: jobs  Lemma: job\n",
      "Actual: savings  Lemma: saving\n",
      "Actual: years  Lemma: year\n",
      "Actual: ones  Lemma: one\n",
      "Actual: has  Lemma: ha\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.stem import WordNetLemmatizer # install wordnet\n",
    "from nltk.stem import PorterStemmer # install punkt\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "word_data = sentence\n",
    "nltk_tokens = nltk.word_tokenize(word_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxnmsRyFr0Ys"
   },
   "source": [
    "__Tokenization__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "colab_type": "code",
    "id": "PtI1c26Nr9Fy",
    "outputId": "597bb114-2dd2-49eb-83bb-bc37b653e6e1"
   },
   "outputs": [],
   "source": [
    "# word token\n",
    "# in nltk_tokens we have every word already separated seperated -> let's throw away punctuation\n",
    "tokenized = []\n",
    "f= open(\"tokenization_Antonio_Curado_Morten_Dahl.txt\",\"w+\")\n",
    "\n",
    "for string in nltk_tokens:\n",
    "    if re.match(r'[a-zA-Z0-9]', string):\n",
    "        tokenized.append(string)\n",
    "        f.write((word + \"\\n\"))\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalization__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the normalization remove hyphens, dots and other punctuation; convert to lowercase\n",
    "normalized = []\n",
    "f= open(\"normalization_Antonio_Curado_Morten_Dahl.txt\",\"w+\")\n",
    "\n",
    "for word in tokenized:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r\"[^\\w\\s]\" ,\"\",word)\n",
    "    normalized.append(word)\n",
    "    f.write((word + \"\\n\"))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMKiN4bxj1LR"
   },
   "source": [
    "__Lemmatization__  \n",
    "_Previous to using both the Stemmer and the Lemmatizer it is required to install  the wordnet and punkt datafiles using the following command: nltk.download()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wordnet Lemmatizer\n",
      "Actual: banks  Lemma: bank\n",
      "Actual: was  Lemma: wa\n",
      "Actual: banks  Lemma: bank\n",
      "Actual: lines  Lemma: line\n",
      "Actual: physicians  Lemma: physician\n",
      "Actual: patients  Lemma: patient\n",
      "Actual: as  Lemma: a\n",
      "Actual: intents  Lemma: intent\n",
      "Actual: purposes  Lemma: purpose\n",
      "Actual: gets  Lemma: get\n",
      "Actual: gets  Lemma: get\n",
      "Actual: physicians  Lemma: physician\n",
      "Actual: was  Lemma: wa\n",
      "Actual: years  Lemma: year\n",
      "Actual: as  Lemma: a\n",
      "Actual: jobs  Lemma: job\n",
      "Actual: savings  Lemma: saving\n",
      "Actual: years  Lemma: year\n",
      "Actual: ones  Lemma: one\n",
      "Actual: has  Lemma: ha\n",
      "Actual: banks  Lemma: bank\n"
     ]
    }
   ],
   "source": [
    "f= open(\"lemmatization_Antonio_Curado_Morten_Dahl.txt\",\"w+\")\n",
    "\n",
    "print(\"\\nWordnet Lemmatizer\")\n",
    "for w in normalized:\n",
    "       if w != wordnet_lemmatizer.lemmatize(w):\n",
    "        print(\"Actual: %s  Lemma: %s\"  % (w,wordnet_lemmatizer.lemmatize(w)))\n",
    "        f.write(\"Actual: %s  Lemma: %s \\n\"  % (w,wordnet_lemmatizer.lemmatize(w)))\n",
    "        \n",
    "f.close()\n",
    "\n",
    "# inspired from https://www.tutorialspoint.com/python/python_stemming_and_lemmatization.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdNn6_nxj3iW"
   },
   "source": [
    "__Stemming__  \n",
    "_Previous to using both the Stemmer and the Lemmatizer it is required to install  the wordnet and punkt datafiles using the following command: nltk.download()_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2136
    },
    "colab_type": "code",
    "id": "QnvI72ANjC6h",
    "outputId": "abefc2e3-e23a-46bd-be2a-94e657e45b15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer\n",
      "Actual: banks  Stem: bank\n",
      "Actual: update  Stem: updat\n",
      "Actual: was  Stem: wa\n",
      "Actual: this  Stem: thi\n",
      "Actual: banks  Stem: bank\n",
      "Actual: organization  Stem: organ\n",
      "Actual: computer  Stem: comput\n",
      "Actual: science  Stem: scienc\n",
      "Actual: lines  Stem: line\n",
      "Actual: article  Stem: articl\n",
      "Actual: odonnell  Stem: odonnel\n",
      "Actual: writes  Stem: write\n",
      "Actual: posting  Stem: post\n",
      "Actual: physicians  Stem: physician\n",
      "Actual: literature  Stem: literatur\n",
      "Actual: steere  Stem: steer\n",
      "Actual: patients  Stem: patient\n",
      "Actual: correctly  Stem: correctli\n",
      "Actual: diagnosed  Stem: diagnos\n",
      "Actual: treated  Stem: treat\n",
      "Actual: why  Stem: whi\n",
      "Actual: steere  Stem: steer\n",
      "Actual: doing  Stem: do\n",
      "Actual: this  Stem: thi\n",
      "Actual: acting  Stem: act\n",
      "Actual: discoverer  Stem: discover\n",
      "Actual: intents  Stem: intent\n",
      "Actual: purposes  Stem: purpos\n",
      "Actual: famous  Stem: famou\n",
      "Actual: gets  Stem: get\n",
      "Actual: famous  Stem: famou\n",
      "Actual: steere  Stem: steer\n",
      "Actual: gets  Stem: get\n",
      "Actual: motive  Stem: motiv\n",
      "Actual: easy  Stem: easi\n",
      "Actual: physicians  Stem: physician\n",
      "Actual: everything  Stem: everyth\n",
      "Actual: everything  Stem: everyth\n",
      "Actual: involved  Stem: involv\n",
      "Actual: computer  Stem: comput\n",
      "Actual: engineer  Stem: engin\n",
      "Actual: was  Stem: wa\n",
      "Actual: building  Stem: build\n",
      "Actual: computer  Stem: comput\n",
      "Actual: manufacturing  Stem: manufactur\n",
      "Actual: company  Stem: compani\n",
      "Actual: several  Stem: sever\n",
      "Actual: years  Stem: year\n",
      "Actual: neartotal  Stem: neartot\n",
      "Actual: disability  Stem: disabl\n",
      "Actual: partially  Stem: partial\n",
      "Actual: company  Stem: compani\n",
      "Actual: failed  Stem: fail\n",
      "Actual: taking  Stem: take\n",
      "Actual: jobs  Stem: job\n",
      "Actual: savings  Stem: save\n",
      "Actual: everything  Stem: everyth\n",
      "Actual: worked  Stem: work\n",
      "Actual: years  Stem: year\n",
      "Actual: lucky  Stem: lucki\n",
      "Actual: ones  Stem: one\n",
      "Actual: foundation  Stem: foundat\n",
      "Actual: fulltime  Stem: fulltim\n",
      "Actual: persistent  Stem: persist\n",
      "Actual: infection  Stem: infect\n",
      "Actual: variety  Stem: varieti\n",
      "Actual: sypmtoms  Stem: sypmtom\n",
      "Actual: try  Stem: tri\n",
      "Actual: literature  Stem: literatur\n",
      "Actual: has  Stem: ha\n",
      "Actual: happened  Stem: happen\n",
      "Actual: necessarily  Stem: necessarili\n",
      "Actual: objective  Stem: object\n",
      "Actual: source  Stem: sourc\n",
      "Actual: information  Stem: inform\n",
      "Actual: focussed  Stem: focuss\n",
      "Actual: this  Stem: thi\n",
      "Actual: emotionally  Stem: emot\n",
      "Actual: involved  Stem: involv\n",
      "Actual: advising  Stem: advis\n",
      "Actual: people  Stem: peopl\n",
      "Actual: certainly  Stem: certainli\n",
      "Actual: advocacy  Stem: advocaci\n",
      "Actual: people  Stem: peopl\n",
      "Actual: very  Stem: veri\n",
      "Actual: effective  Stem: effect\n",
      "Actual: banks  Stem: bank\n",
      "Actual: skepticism  Stem: skeptic\n",
      "Actual: chastity  Stem: chastiti\n",
      "Actual: shameful  Stem: shame\n",
      "Actual: surrender  Stem: surrend\n"
     ]
    }
   ],
   "source": [
    "f= open(\"lemmatization_Antonio_Curado_Morten_Dahl.txt\",\"w+\")\n",
    "\n",
    "print(\"\\nPorter Stemmer\")\n",
    "for w in normalized:\n",
    "       if w != porter_stemmer.stem(w):\n",
    "        print(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
    "        f.write(\"Actual: %s  Stem: %s\"  % (w,porter_stemmer.stem(w)))\n",
    "        \n",
    "f.close()\n",
    "        \n",
    "# inspired from https://www.tutorialspoint.com/python/python_stemming_and_lemmatization.htm"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of TextMining1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
